{
    "paragraphs": [
        {
            "page_no": 1,
            "text": "The document outlines a Deep Learning Assignment for a 4th Year B.Tech Intern. The objective is to understand and apply the basics of Deep Learning concepts, focusing on prevalent models like Transformers, decoder-based Large Language Models (LLMs), and embedding layers. The assignment is designed to be completed over 4 weeks and involves both theoretical and practical work. General instructions include using free GPU resources such as Google Colab for all coding tasks, organizing work in weekly modules, and submitting a final report and code notebook at the end of the 4 weeks."
        },
        {
            "page_no": 1,
            "text": "Week 1 focuses on the Basics of Deep Learning. Theoretical tasks include explaining the difference between Machine Learning and Deep Learning, defining activation functions with examples, describing the architecture of a basic feedforward neural network, and explaining backpropagation. The practical task involves implementing a simple feedforward neural network using PyTorch or TensorFlow on Google Colab."
        },
        {
            "page_no": 1,
            "text": "Week 2 covers Embeddings and Attention. Theoretical tasks delve into what an embedding layer is and why it's used, explaining word embeddings with examples like Word2Vec and GloVe, and detailing the attention mechanism and its significance. Practical tasks include implementing an embedding layer for a given vocabulary, visualizing embeddings using PCA or t-SNE, and implementing a scaled dot-product attention mechanism in PyTorch/TensorFlow."
        },
        {
            "page_no": 1,
            "text": "Week 3 explores Transformers and LLM Architecture. Theoretical tasks involve explaining the Transformer model architecture, defining self-attention and its importance, comparing Transformers with RNNs and CNNs for sequential data, and understanding the role of the decoder in Transformer-based LLMs like GPT. The practical task is to build a small Transformer encoder or decoder block and train it on a toy dataset, such as for character-level language modeling."
        },
        {
            "page_no": 2,
            "text": "Week 4 deals with Working with Pre-trained Models. Theoretical tasks include defining Large Language Models with examples like GPT and LLaMA, and explaining the difference between encoder-decoder and decoder-only models. The practical tasks involve using the Hugging Face Transformers library to load a pre-trained GPT model and generate text based on a given prompt."
        },
        {
            "page_no": 2,
            "text": "For submission, students are required to submit a Jupyter Notebook (Google Colab) with all implementations and a PDF report. The PDF report should explain theory answers, implementation steps, and observations. The overall deadline for the assignment is 4 Weeks from the date of assignment."
        },
        {
            "page_no": 2,
            "text": "Additional resources provided include the 'Attention Is All You Need' paper, Hugging Face Transformers Documentation, 'Deep Learning with PyTorch' documentation, and a Google Colab Guide."
        }
    ],
    "tables": [],
    "relations": [
        {
            "subject": "Deep Learning Assignment",
            "predicate": "is_for",
            "object": "4th Year B.Tech Intern",
            "page_no": 1
        },
        {
            "subject": "Deep Learning Assignment",
            "predicate": "covers_concept",
            "object": "Deep Learning",
            "page_no": 1
        },
        {
            "subject": "Deep Learning Assignment",
            "predicate": "covers_concept",
            "object": "Transformers",
            "page_no": 1
        },
        {
            "subject": "Deep Learning Assignment",
            "predicate": "covers_concept",
            "object": "Large Language Models",
            "page_no": 1
        },
        {
            "subject": "Deep Learning Assignment",
            "predicate": "covers_concept",
            "object": "Embedding Layers",
            "page_no": 1
        },
        {
            "subject": "Deep Learning Assignment",
            "predicate": "has_duration",
            "object": "4 weeks",
            "page_no": 1
        },
        {
            "subject": "Deep Learning Assignment",
            "predicate": "involves",
            "object": "theoretical work",
            "page_no": 1
        },
        {
            "subject": "Deep Learning Assignment",
            "predicate": "involves",
            "object": "practical work",
            "page_no": 1
        },
        {
            "subject": "Practical tasks",
            "predicate": "uses_resource",
            "object": "Google Colab",
            "page_no": 1
        },
        {
            "subject": "Work",
            "predicate": "organized_into",
            "object": "weekly modules",
            "page_no": 1
        },
        {
            "subject": "Assignment submission",
            "predicate": "includes",
            "object": "final report",
            "page_no": 1
        },
        {
            "subject": "Assignment submission",
            "predicate": "includes",
            "object": "code notebook",
            "page_no": 1
        },
        {
            "subject": "Week 1",
            "predicate": "focuses_on",
            "object": "Basics of Deep Learning",
            "page_no": 1
        },
        {
            "subject": "Week 1 Theoretical Tasks",
            "predicate": "discusses",
            "object": "Machine Learning",
            "page_no": 1
        },
        {
            "subject": "Week 1 Theoretical Tasks",
            "predicate": "discusses",
            "object": "Deep Learning",
            "page_no": 1
        },
        {
            "subject": "Week 1 Theoretical Tasks",
            "predicate": "discusses",
            "object": "activation functions",
            "page_no": 1
        },
        {
            "subject": "Week 1 Theoretical Tasks",
            "predicate": "discusses",
            "object": "feedforward neural network",
            "page_no": 1
        },
        {
            "subject": "Week 1 Theoretical Tasks",
            "predicate": "discusses",
            "object": "backpropagation",
            "page_no": 1
        },
        {
            "subject": "Week 1 Practical Task",
            "predicate": "involves_implementation_of",
            "object": "feedforward neural network",
            "page_no": 1
        },
        {
            "subject": "Feedforward neural network implementation",
            "predicate": "uses_framework",
            "object": "PyTorch",
            "page_no": 1
        },
        {
            "subject": "Feedforward neural network implementation",
            "predicate": "uses_framework",
            "object": "TensorFlow",
            "page_no": 1
        },
        {
            "subject": "Week 2",
            "predicate": "focuses_on",
            "object": "Embeddings and Attention",
            "page_no": 1
        },
        {
            "subject": "Week 2 Theoretical Tasks",
            "predicate": "discusses",
            "object": "embedding layer",
            "page_no": 1
        },
        {
            "subject": "Week 2 Theoretical Tasks",
            "predicate": "discusses",
            "object": "word embeddings",
            "page_no": 1
        },
        {
            "subject": "Word embeddings",
            "predicate": "has_example",
            "object": "Word2Vec",
            "page_no": 1
        },
        {
            "subject": "Word embeddings",
            "predicate": "has_example",
            "object": "GloVe",
            "page_no": 1
        },
        {
            "subject": "Week 2 Theoretical Tasks",
            "predicate": "discusses",
            "object": "attention mechanism",
            "page_no": 1
        },
        {
            "subject": "Week 2 Practical Tasks",
            "predicate": "involves_implementation_of",
            "object": "embedding layer",
            "page_no": 1
        },
        {
            "subject": "Week 2 Practical Tasks",
            "predicate": "involves_visualization_using",
            "object": "PCA",
            "page_no": 1
        },
        {
            "subject": "Week 2 Practical Tasks",
            "predicate": "involves_visualization_using",
            "object": "t-SNE",
            "page_no": 1
        },
        {
            "subject": "Week 2 Practical Tasks",
            "predicate": "involves_implementation_of",
            "object": "scaled dot-product attention",
            "page_no": 1
        },
        {
            "subject": "Scaled dot-product attention implementation",
            "predicate": "uses_framework",
            "object": "PyTorch",
            "page_no": 1
        },
        {
            "subject": "Scaled dot-product attention implementation",
            "predicate": "uses_framework",
            "object": "TensorFlow",
            "page_no": 1
        },
        {
            "subject": "Week 3",
            "predicate": "focuses_on",
            "object": "Transformers and LLM Architecture",
            "page_no": 1
        },
        {
            "subject": "Week 3 Theoretical Tasks",
            "predicate": "discusses",
            "object": "Transformer model architecture",
            "page_no": 1
        },
        {
            "subject": "Week 3 Theoretical Tasks",
            "predicate": "discusses",
            "object": "self-attention",
            "page_no": 1
        },
        {
            "subject": "Week 3 Theoretical Tasks",
            "predicate": "compares",
            "object": "Transformers",
            "page_no": 1
        },
        {
            "subject": "Week 3 Theoretical Tasks",
            "predicate": "compares",
            "object": "RNNs",
            "page_no": 1
        },
        {
            "subject": "Week 3 Theoretical Tasks",
            "predicate": "compares",
            "object": "CNNs",
            "page_no": 1
        },
        {
            "subject": "Week 3 Theoretical Tasks",
            "predicate": "discusses",
            "object": "decoder in Transformer-based LLMs",
            "page_no": 1
        },
        {
            "subject": "Transformer-based LLMs",
            "predicate": "has_example",
            "object": "GPT",
            "page_no": 1
        },
        {
            "subject": "Week 3 Practical Tasks",
            "predicate": "involves_building",
            "object": "Transformer encoder or decoder block",
            "page_no": 1
        },
        {
            "subject": "Transformer encoder or decoder block",
            "predicate": "trained_on",
            "object": "toy dataset",
            "page_no": 1
        },
        {
            "subject": "Toy dataset",
            "predicate": "example_application",
            "object": "character-level language modeling",
            "page_no": 1
        },
        {
            "subject": "Week 4",
            "predicate": "focuses_on",
            "object": "Working with Pre-trained Models",
            "page_no": 2
        },
        {
            "subject": "Week 4 Theoretical Tasks",
            "predicate": "discusses",
            "object": "Large Language Models",
            "page_no": 2
        },
        {
            "subject": "Large Language Models",
            "predicate": "has_example",
            "object": "GPT",
            "page_no": 2
        },
        {
            "subject": "Large Language Models",
            "predicate": "has_example",
            "object": "LLaMA",
            "page_no": 2
        },
        {
            "subject": "Week 4 Theoretical Tasks",
            "predicate": "discusses",
            "object": "encoder-decoder models",
            "page_no": 2
        },
        {
            "subject": "Week 4 Theoretical Tasks",
            "predicate": "discusses",
            "object": "decoder-only models",
            "page_no": 2
        },
        {
            "subject": "Week 4 Practical Tasks",
            "predicate": "uses_library",
            "object": "Hugging Face Transformers library",
            "page_no": 2
        },
        {
            "subject": "Week 4 Practical Tasks",
            "predicate": "involves",
            "object": "loading pre-trained GPT model",
            "page_no": 2
        },
        {
            "subject": "Week 4 Practical Tasks",
            "predicate": "involves",
            "object": "generating text based on prompt",
            "page_no": 2
        },
        {
            "subject": "Submission requirements",
            "predicate": "include",
            "object": "Jupyter Notebook",
            "page_no": 2
        },
        {
            "subject": "Jupyter Notebook",
            "predicate": "contains",
            "object": "all implementations",
            "page_no": 2
        },
        {
            "subject": "Jupyter Notebook",
            "predicate": "is_on",
            "object": "Google Colab",
            "page_no": 2
        },
        {
            "subject": "Submission requirements",
            "predicate": "include",
            "object": "PDF report",
            "page_no": 2
        },
        {
            "subject": "PDF report",
            "predicate": "explains",
            "object": "theory answers",
            "page_no": 2
        },
        {
            "subject": "PDF report",
            "predicate": "explains",
            "object": "implementation steps",
            "page_no": 2
        },
        {
            "subject": "PDF report",
            "predicate": "explains",
            "object": "observations",
            "page_no": 2
        },
        {
            "subject": "Assignment",
            "predicate": "has_deadline",
            "object": "4 Weeks from date of assignment",
            "page_no": 2
        },
        {
            "subject": "Additional Resources",
            "predicate": "include",
            "object": "Attention Is All You Need",
            "page_no": 2
        },
        {
            "subject": "Additional Resources",
            "predicate": "include",
            "object": "Hugging Face Transformers Documentation",
            "page_no": 2
        },
        {
            "subject": "Additional Resources",
            "predicate": "include",
            "object": "Deep Learning with PyTorch",
            "page_no": 2
        },
        {
            "subject": "Additional Resources",
            "predicate": "include",
            "object": "Google Colab Guide",
            "page_no": 2
        },
        {
            "subject": "Attention Is All You Need",
            "predicate": "is_type",
            "object": "Transformer Paper",
            "page_no": 2
        }
    ]
}